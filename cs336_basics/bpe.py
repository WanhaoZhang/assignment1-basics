import os
import regex as re
from typing import Dict, List, Tuple
from collections import Counter

# GPT-2 çš„é¢„åˆ†è¯æ­£åˆ™è¡¨è¾¾å¼
GPT2_SPLIT_PATTERN = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""

def train_bpe(
    input_path: str | os.PathLike,
    vocab_size: int,
    special_tokens: list[str],
    **kwargs,
) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
    """
    Given the path to an input corpus, run train a BPE tokenizer and
    output its vocabulary and merges.
    """
    
    # ==========================================
    # 1. æ•°æ®åŠ è½½
    # ==========================================
    # TODO: è¯»å– input_path å¯¹åº”çš„æ–‡ä»¶å†…å®¹
    # æç¤º: ä½¿ç”¨ open(input_path, "r", encoding="utf-8")
    with open(input_path, "r", encoding="utf-8") as file:
        text = file.read()
    # print("ZHANG --------- "+text[:10])
    
    # ==========================================
    # 2. é¢„å¤„ç† (Pre-tokenization)
    # ==========================================
    # TODO: å®ç° _pretokenize_and_count å‡½æ•°
    # æç¤ºï¼šåŠ¡å¿…å…ˆå¤„ç† special_tokensï¼Œå†è¿›è¡Œæ­£åˆ™åˆ‡åˆ†
    word_counts = _pretokenize_and_count(text, special_tokens)
    # print("first time")
    # for word, freq in word_counts.items():
    #     print(f"{word}, {freq}\n")
            
    # ==========================================
    # 3. åˆå§‹åŒ–è¯è¡¨
    # ==========================================
    # åˆå§‹è¯è¡¨åŒ…å« 256 ä¸ªå­—èŠ‚ (0-255)
    vocab: Dict[int, bytes] = {i: bytes([i]) for i in range(256)}
    
    # è®¡ç®—éœ€è¦æ‰§è¡Œå¤šå°‘æ¬¡åˆå¹¶æ“ä½œ
    # å…¬å¼ï¼šç›®æ ‡å¤§å° - åŸºç¡€å­—ç¬¦(256) - ç‰¹æ®ŠTokenæ•°é‡
    num_merges = vocab_size - 256 - len(special_tokens)
    merges: List[Tuple[bytes, bytes]] = []

    # ==========================================
    # 4. BPE è®­ç»ƒå¾ªç¯
    # ==========================================
    next_token_id = 256
    
    for i in range(num_merges):
        # 4.1 ç»Ÿè®¡å½“å‰æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„é¢‘ç‡
        # TODO: å®ç° _get_pair_stats å‡½æ•°
        pair_counts = _get_pair_stats(word_counts)
        
        if not pair_counts:
            break

        # 4.2 æ‰¾åˆ°é¢‘ç‡æœ€é«˜çš„ Pair
        # TODO: å®ç° Tie-breaking è§„åˆ™
        # è§„åˆ™ï¼šé¢‘ç‡æœ€é«˜ä¼˜å…ˆï¼›å¦‚æœé¢‘ç‡ç›¸åŒï¼Œé€‰å­—å…¸åºæ›´å¤§çš„ (lexicographically greater)
        # æç¤º: max(pair_counts, key=lambda p: (...))
    
        best_pair = max(pair_counts, key=lambda p:(pair_counts[p], vocab[p[0]], vocab[p[1]]))
        
        # 4.3 è®°å½•åˆå¹¶è§„åˆ™
        # æç¤ºï¼šä½ éœ€è¦ä» vocab ä¸­å–å‡º best_pair å¯¹åº”çš„ bytesï¼Œå­˜å…¥ merges åˆ—è¡¨
        # token_bytes_a = ...
        # token_bytes_b = ...
        # merges.append(...)

        token_byte_a = vocab[best_pair[0]]
        token_byte_b = vocab[best_pair[1]]
        merges.append((token_byte_a, token_byte_b))
        
        # 4.4 æ›´æ–°è¯è¡¨
        # å°†æ–°ç”Ÿæˆçš„ token ID (next_token_id) åŠ å…¥ vocab
        # new_token_bytes = ...
        # vocab[next_token_id] = new_token_bytes
        new_token_bytes = token_byte_a + token_byte_b
        vocab[next_token_id] = new_token_bytes
        
        # 4.5 æ›´æ–°ç»Ÿè®¡æ•°æ®
        # TODO: å®ç° _merge_vocab å‡½æ•°
        # å°†æ‰€æœ‰å‡ºç° best_pair çš„åœ°æ–¹æ›¿æ¢ä¸º next_token_id
        word_counts = _merge_vocab(word_counts, best_pair, next_token_id)
        # print('for')
        # for word, freq in word_counts.items():
        #     print(f"{word}, {freq}\n")
        
        next_token_id += 1

    # ==========================================
    # 5. æ·»åŠ  Special Tokens
    # ==========================================
    # TODO: å°† special_tokens æ·»åŠ åˆ°è¯è¡¨æœ«å°¾
    # éå† special_tokensï¼Œç»™å®ƒä»¬åˆ†é… IDï¼Œå¹¶åŠ å…¥ vocab
    for token in special_tokens:
        vocab[next_token_id] = token.encode("utf-8")
        next_token_id += 1

    return vocab, merges


# ==========================================
# è¾…åŠ©å‡½æ•°å®šä¹‰ (éœ€è¦ä½ æ¥å®ç°)
# ==========================================

def _pretokenize_and_count(text: str, special_tokens: list[str]) -> Counter[tuple[int, ...]]:
    """
    å…ˆæŒ‰ special_tokens åˆ‡åˆ†ï¼Œå†åº”ç”¨ GPT-2 æ­£åˆ™ï¼Œæœ€åè½¬ä¸º bytes tuple å¹¶ç»Ÿè®¡é¢‘ç‡ã€‚
    """
    counts = Counter()
    # TODO: ä½ çš„å®ç°ä»£ç 
    # 1. ä½¿ç”¨ re.split å¤„ç† special_tokens (å»ºè®®ç”¨ re.escape)
    # 2. å¯¹åˆ†å‡ºæ¥çš„ç‰‡æ®µä½¿ç”¨ re.findall(GPT2_SPLIT_PATTERN, ...)
    # 3. å°†å•è¯è½¬ä¸º UTF-8 bytes tuple å¹¶å­˜å…¥ counts
    if special_tokens:
        pattern = '|'.join(re.escape(token) for token in special_tokens)
        segs = re.split(f"({pattern})", text)
    else:
        segs = [text]
    # print(segs)
    for seg in segs:
        if not seg or seg in special_tokens:
            continue
        
        words = re.findall(GPT2_SPLIT_PATTERN, seg)
        # print(words)
        for word in words:
            if not word: continue
            word_as_bytes = word.encode("utf-8")
            counts[tuple(word_as_bytes)] += 1
    # for i, (word, freq) in enumerate(counts.items()):
    #     print(f"{bytes(word).decode('utf-8')},{freq}")
    
    return counts

def _get_pair_stats(word_counts: Counter[tuple[int, ...]]) -> Dict[tuple[int, int], int]:
    """
    ç»Ÿè®¡å½“å‰æ‰€æœ‰å•è¯ä¸­ç›¸é‚» token å¯¹çš„å‡ºç°é¢‘ç‡ã€‚
    """
    pair_counts = Counter()
    # TODO: ä½ çš„å®ç°ä»£ç 
    # éå† word_counts ä¸­çš„æ¯ä¸ªå•è¯ (tuple of ints)
    # ç»Ÿè®¡ç›¸é‚»çš„ (item, next_item) å¯¹
    for word, freq in word_counts.items():
        if len(word) < 2:
            continue
        
        for i in range(len(word) - 1):
            pair = (word[i], word[i+1])
            pair_counts[pair] += freq

    # for word_pair, freq in pair_counts.items():
    #     print(f"{word_pair}, {freq}")

    return pair_counts

def _merge_vocab(
    word_counts: Counter[tuple[int, ...]], 
    pair_to_merge: tuple[int, int], 
    new_token_id: int
) -> Counter[tuple[int, ...]]:
    """
    å°† word_counts ä¸­æ‰€æœ‰çš„ pair_to_merge æ›¿æ¢ä¸º new_token_idã€‚
    """
    new_counts = Counter()
    # TODO: ä½ çš„å®ç°ä»£ç 
    # éå† word_counts
    # åœ¨å•è¯ä¸­æ‰¾åˆ° pair_to_merge å¹¶æ›¿æ¢ä¸º new_token_id
    # æç¤º: å¦‚æœå•è¯å¤ªçŸ­æˆ–è€…ä¸åŒ…å« pair çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œå¯ä»¥ç›´æ¥è·³è¿‡ä»¥ä¼˜åŒ–é€Ÿåº¦
    for word, freq in word_counts.items():
        if len(word) < 2:
            new_counts[word] += freq
            continue
        if pair_to_merge[0] not in word:
            new_counts[word] += freq
            continue
        
        new_word = list(word)
        i = 0
        while i < len(new_word) - 1:
            cur_pair = (new_word[i], new_word[i+1])
            if cur_pair == pair_to_merge:
                new_word[i:i+2] = [new_token_id]
            else:
                i += 1
                
        new_counts[tuple(new_word)] += freq
    
    return new_counts


# ==========================================
# æµ‹è¯•ä»£ç  (æ ¹æ® PDF ç¬¬ 7 é¡µçš„ä¾‹å­)
# ==========================================
if __name__ == "__main__":
    import tempfile
    import os
    
    print("="*40)
    print("  CS336 Assignment 1: BPE å…¨åŠŸèƒ½è°ƒè¯•")
    print("="*40)

    # 1. æ„é€ æµ‹è¯•æ•°æ® 
    # åŸºç¡€æ•°æ®ï¼šlow(5), lower(2), newest(6), widest(3)
    # ç‰¹æ®Šè®¾è®¡ï¼š
    #   æˆ‘ä»¬åœ¨ä¸­é—´æ’å…¥ "<|endoftext|>"ã€‚
    #   å¦‚æœä½ çš„ regex/special token å¤„ç†æ­£ç¡®ï¼Œå®ƒåº”è¯¥è¢«è§†ä¸ºä¸€ä¸ªæ•´ä½“ï¼Œä¸”ä¸å½±å“å‰åå•è¯çš„ç»Ÿè®¡ã€‚
    special_token_str = "<|endoftext|>"
    
    # æ„é€ æ–‡æœ¬
    # æ³¨æ„ï¼šå•è¯é—´åŠ ç©ºæ ¼æ˜¯ä¸ºäº†è®© GPT-2 regex æ­£ç¡®è¯†åˆ«ä¸ºå•è¯
    base_text = "low " * 5 + "lower " * 2 + "newest " * 6 + "widest " * 3
    input_text = base_text + special_token_str + " " + base_text # ç¿»å€æ•°æ®é‡ï¼Œå¤¹æ‚ç‰¹æ®Štoken
    
    print(f"[è¾“å…¥é¢„è§ˆ]: {input_text[:60]}...")
    print(f"[ç‰¹æ®ŠToken]: {special_token_str}")

    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode="w", delete=False, encoding="utf-8") as tmp_file:
        tmp_file.write(input_text)
        tmp_path = tmp_file.name

    try:
        # 2. è®¾ç½®å‚æ•°
        # åˆå§‹å­—èŠ‚: 256 ä¸ª
        # è®¡åˆ’ Merges: 3 æ¬¡ (ä¸ºäº†éªŒè¯ st -> est -> ow/lo)
        # ç‰¹æ®Š Token: 1 ä¸ª
        # æ€» Vocab Size = 256 + 3 + 1 = 260
        target_vocab_size = 260
        
        print(f"\n[å¼€å§‹è®­ç»ƒ] ç›®æ ‡è¯è¡¨å¤§å°: {target_vocab_size}")
        
        vocab, merges = train_bpe(
            input_path=tmp_path,
            vocab_size=target_vocab_size,
            special_tokens=[special_token_str]
        )
        
        print("\n" + "-"*20 + " è®­ç»ƒç»“æœ " + "-"*20)

        # 3. éªŒè¯ Merges (æ ¸å¿ƒç®—æ³•é€»è¾‘)
        print(f"ç”Ÿæˆçš„ Merges ({len(merges)}ä¸ª):")
        for i, (b1, b2) in enumerate(merges):
            print(f"  {i+1}. {b1} + {b2}")

        # [cite: 196] æ£€æŸ¥ç‚¹ 1: é¢‘ç‡æœ€é«˜ä¸”å­—å…¸åºæœ€å¤§
        # 's' + 't' (9+9=18æ¬¡) vs 'e' + 's' (9+9=18æ¬¡)
        # å­—å…¸åº 't' > 's' (æ¯”è¾ƒç¬¬äºŒä¸ªå…ƒç´ )ï¼Œæˆ–è€…æ•´ä½“æ¯”è¾ƒ ('s', 't') > ('e', 's')
        # é¢„æœŸå† å†›: ('s', 't') -> b'st'
        if len(merges) > 0:
            assert merges[0] == (b's', b't'), \
                f"âŒ Merge 1 é”™è¯¯! æœŸæœ› (b's', b't'), å®é™… {merges[0]} (æ£€æŸ¥å­—å…¸åº Tie-breaking)"
            print("âœ… Merge 1 é€šè¿‡: ('s', 't')")

        # [cite: 198] æ£€æŸ¥ç‚¹ 2: çº§è”åˆå¹¶
        # ä¸Šä¸€æ­¥ç”Ÿæˆäº† b'st'ã€‚è¿™ä¸€æ­¥åº”è¯¥æ˜¯ 'e' + b'st'
        if len(merges) > 1:
            assert merges[1] == (b'e', b'st'), \
                f"âŒ Merge 2 é”™è¯¯! æœŸæœ› (b'e', b'st'), å®é™… {merges[1]}"
            print("âœ… Merge 2 é€šè¿‡: ('e', 'st')")

        # æ£€æŸ¥ç‚¹ 3: æ£€æŸ¥ 'o' å’Œ 'w'
        # åœ¨ 'low' å’Œ 'lower' ä¸­ï¼Œ'l'+'o' å’Œ 'o'+'w' é¢‘ç‡ç›¸åŒã€‚
        # æ¯”è¾ƒ tuple(b'l', b'o') vs tuple(b'o', b'w')
        # b'l'(108) < b'o'(111)ã€‚æ‰€ä»¥ ('o', 'w') å­—å…¸åºæ›´å¤§ï¼Œåº”è¯¥å…ˆåˆå¹¶ã€‚
        if len(merges) > 2:
            if merges[2] == (b'o', b'w'):
                 print("âœ… Merge 3 é€šè¿‡: ('o', 'w') (å­—å…¸åºèƒœå‡º)")
            elif merges[2] == (b'l', b'o'):
                 print("âš ï¸ Merge 3 æ˜¯ ('l', 'o')ã€‚è¿™åœ¨é¢‘ç‡ç›¸åŒæ—¶æ˜¯å­—å…¸åºè¾ƒå°çš„ï¼Œè¯·æ£€æŸ¥ä½ çš„ max key é€»è¾‘ã€‚")
            else:
                 print(f"â“ Merge 3 æ˜¯ {merges[2]}")

        # 4. éªŒè¯ Special Token (ID åˆ†é…)
        # é€»è¾‘ï¼š256 (base) + 3 (merges) = 259 ä¸ªä½ç½® (ID 0-258)
        # Special Token åº”è¯¥æ˜¯ç¬¬ 260 ä¸ªä½ç½® (ID 259)
        # [cite: 235] Special tokens do not affect BPE training (added at end)
        print("\n" + "-"*20 + " Special Tokens " + "-"*20)
        expected_st_id = 256 + 3  # = 259
        
        if expected_st_id in vocab:
            st_content = vocab[expected_st_id]
            print(f"ID {expected_st_id}: {st_content}")
            
            assert st_content == special_token_str.encode("utf-8"), \
                f"âŒ Special Token å†…å®¹é”™è¯¯! æœŸæœ› {special_token_str.encode('utf-8')}, å®é™… {st_content}"
            print("âœ… Special Token å†…å®¹ä¸ä½ç½®æ­£ç¡®")
        else:
            print(f"âŒ è¯è¡¨ä¸­æ‰¾ä¸åˆ° ID {expected_st_id}ï¼Œè¯·æ£€æŸ¥ vocab_size è®¡ç®—é€»è¾‘")

        # 5. éªŒè¯æ˜¯å¦è¢«é”™è¯¯åˆ‡åˆ†
        # æ£€æŸ¥ vocab é‡Œæ˜¯å¦æœ‰ '<' æˆ– '|' è¿™ç§è¢«åˆ‡ç¢çš„ç—•è¿¹ï¼Ÿ
        # ç®€å•æ£€æŸ¥ï¼šçœ‹ special token çš„ ID æ˜¯å¦æ˜¯ç‹¬ç«‹åˆ†é…çš„
        print("\nğŸ‰ å…¨åŠŸèƒ½æµ‹è¯•å®Œæˆï¼å¦‚æœæ²¡æœ‰çº¢è‰²çš„âŒï¼Œè¯´æ˜ä½ çš„ BPE è®­ç»ƒå™¨é€»è¾‘å®Œç¾ï¼")

    except NotImplementedError:
        print("\nâš ï¸  ä»£ç æœªå®Œæˆ (NotImplementedError)")
    except AssertionError as e:
        print(f"\nâŒ æ–­è¨€å¤±è´¥: {e}")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œæ—¶é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†
        if os.path.exists(tmp_path):
            os.remove(tmp_path)